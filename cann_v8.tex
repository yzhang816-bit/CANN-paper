

\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{subcaption}
\usepackage{geometry}

\usepackage{multirow} 
\geometry{margin=1in}

% Define theorem environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{remark}{Remark}[section]

% Title and author
\title{Curvature-Aware Neural Networks for Robust Graph Representation Learning}
\author{Anonymous Authors}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Graph-structured data pervades modern machine learning applications, from social networks and recommender systems to molecular biology and neuroscience. While Graph Neural Networks (GNNs) have achieved state-of-the-art performance on numerous graph-related tasks, they predominantly operate under the implicit assumption that the underlying graph geometry is Euclidean. This assumption is frequently violated in real-world graphs, which often exhibit non-Euclidean, hyperbolic-like structures characterized by hierarchical organization, tree-like connectivity patterns, and scale-free properties. We propose Curvature-Aware Neural Networks (CANNs), a novel class of GNNs that explicitly model the local geometric curvature of graphs. By leveraging discrete Ollivier-Ricci curvature from differential geometry, our model adapts its message-passing scheme to respect the underlying geometric structure, yielding more robust and expressive representations. Through comprehensive experiments on both synthetic and real-world datasets spanning citation networks, social graphs, and molecular structures, we demonstrate that CANNs consistently outperform existing GNN architectures by 2-3\% in accuracy while exhibiting 4-5\% improved robustness against adversarial perturbations. Our approach maintains computational complexity comparable to standard GNNs while providing enhanced interpretability through explicit geometric reasoning.
\end{abstract}

\section{Introduction}

Graphs provide a powerful and flexible framework for representing complex relationships between entities in numerous domains. In recent years, Graph Neural Networks (GNNs) have emerged as the dominant paradigm for learning on graph-structured data \cite{kipf2017semi,velickovic2018graph,hamilton2017inductive,xu2019powerful}. The fundamental principle underlying GNNs involves learning node representations through iterative aggregation of information from local neighborhoods via message-passing mechanisms. This approach has proven highly effective across diverse tasks including node classification, link prediction, and graph-level prediction.

\subsection{Motivation: The Hidden Geometry Problem}

Despite their empirical success, most existing GNN architectures implicitly assume Euclidean geometry when processing graph data. Specifically, they treat all neighborhoods as structurally equivalent and apply uniform aggregation schemes regardless of local topological properties. This Euclidean assumption manifests in several ways: nodes are embedded in flat Euclidean spaces, distance metrics are based on $\ell_2$ norms, and message-passing mechanisms make no distinction between densely connected communities and sparse tree-like regions.

However, accumulating evidence from network science demonstrates that real-world graphs frequently violate this Euclidean assumption. Consider the following prominent examples:

\textbf{Social Networks:} Online social platforms such as Facebook, Twitter, and LinkedIn exhibit pronounced hierarchical community structures with power-law degree distributions. Influential users occupy central positions in tree-like propagation structures, while local communities form densely connected clusters. These properties are fundamentally hyperbolic rather than Euclidean in nature \cite{krioukov2010hyperbolic}.

\textbf{Biological Networks:} Protein-protein interaction networks display tree-like connectivity patterns reflecting evolutionary relationships. Metabolic pathways organize hierarchically, with central metabolites serving as hubs connecting numerous reactions. Such structures naturally embed in negatively curved spaces.

\textbf{Knowledge Graphs:} Semantic relationships in knowledge bases form hierarchical taxonomies, where concepts organize into tree-like subsumption hierarchies. The "is-a" relationships create natural hierarchies poorly captured by Euclidean embeddings.

\textbf{Citation Networks:} Academic citation patterns demonstrate temporal hierarchies where seminal papers spawn entire research subtrees. Topic-based clustering combined with cross-disciplinary citations creates mixed geometric properties within a single graph.

These observations suggest that the geometry of real-world graphs is inherently non-Euclidean, often exhibiting characteristics of hyperbolic spaces where distances grow exponentially with radius. In hyperbolic geometry, the volume of a ball of radius $r$ grows as $e^r$ rather than $r^d$ as in Euclidean spaces, naturally accommodating the exponential growth patterns observed in hierarchical networks.

\subsection{Research Gap and Our Contribution}

While recent work in geometric deep learning has explored non-Euclidean geometries, existing approaches suffer from significant limitations. Hyperbolic Graph Convolutional Networks (HGCNs) \cite{chami2019hyperbolic} operate entirely in hyperbolic space, demonstrating promise for hierarchical graphs. However, they assume \emph{uniform negative curvature} throughout the entire graph. This assumption is overly restrictive, as real-world networks exhibit \emph{mixed curvature}: densely connected communities possess positive curvature (sphere-like geometry), sparse tree-like regions have negative curvature (hyperbolic geometry), and transition zones may be approximately flat (Euclidean geometry).

Graph curvature measures such as Ollivier-Ricci and Forman-Ricci curvature have been successfully applied to characterize graph properties, identify community structures, and detect anomalies in network science \cite{ollivier2009ricci,ni2015ricci}. However, these geometric quantities have not been systematically integrated into deep learning architectures for end-to-end representation learning.

We address this gap by introducing \textbf{Curvature-Aware Neural Networks (CANNs)}, a principled framework that:

\begin{enumerate}
   \item \textbf{Explicitly models local geometry} by computing discrete Ricci curvature at each node to quantify local geometric properties without assuming uniform curvature.
   
   \item \textbf{Adapts message-passing dynamically} by modulating information aggregation based on local curvature, focusing attention on immediate neighbors in densely connected regions while aggregating more broadly in tree-like structures.
   
   \item \textbf{Learns geometry-aware representations} through curvature-dependent attention mechanisms that respect the intrinsic geometric structure of graphs.
\end{enumerate}

Our key insight is that respecting the intrinsic geometry of graphs enables better generalization, improved robustness to adversarial perturbations, and enhanced interpretability. By making geometric assumptions explicit rather than implicit, CANNs can adapt their behavior to the actual structure of the data.

\subsection{Main Contributions}

We summarize our primary contributions as follows:

\begin{enumerate}
   \item \textbf{Novel Architecture:} We introduce CANN, a practical GNN architecture with efficient curvature computation and geometry-aware aggregation that adapts to mixed-curvature graph structures.
   
   \item \textbf{Algorithmic Framework:} We provide complete algorithmic descriptions with detailed pseudocode that can be directly implemented, including efficient computation of Ollivier-Ricci curvature and curvature-aware message passing.
   
   \item \textbf{Theoretical Analysis:} We analyze the computational complexity of CANNs, demonstrating that our approach maintains the same asymptotic complexity $O(L \cdot m \cdot d^2)$ as standard GNNs, making it practical for large-scale applications.
   
   \item \textbf{Comprehensive Evaluation:} We conduct extensive experiments across synthetic and real-world datasets, demonstrating consistent improvements of 2-3\% in predictive accuracy over state-of-the-art baselines including GCN, GAT, GraphSAGE, GIN, and HGCN.
   
   \item \textbf{Robustness Analysis:} We provide rigorous evaluation of adversarial robustness, showing CANNs maintain 4-5\% higher accuracy under various perturbation attacks compared to existing methods.
\end{enumerate}

The remainder of this paper is organized as follows. Section 2 reviews related work in graph neural networks, geometric deep learning, and graph curvature. Section 3 provides essential background on discrete graph curvature. Section 4 presents our proposed CANN architecture with detailed algorithms. Section 5 reports comprehensive experimental results. Section 6 discusses implications, limitations, and future directions. Section 7 concludes.

\section{Related Work}

\subsection{Graph Neural Networks}

Graph Neural Networks have become the standard approach for learning on graph-structured data. The seminal work of Kipf and Welling \cite{kipf2017semi} introduced Graph Convolutional Networks (GCNs), which perform spectral convolution operations on graphs through localized first-order approximations of graph Laplacians. Subsequently, Graph Attention Networks (GATs) \cite{velickovic2018graph} incorporated attention mechanisms to learn adaptive importance weights for neighbors, GraphSAGE \cite{hamilton2017inductive} introduced sampling-based inductive learning, and Graph Isomorphism Networks (GINs) \cite{xu2019powerful} achieved maximal expressive power for distinguishing graph structures.

Despite their architectural differences, these models share a fundamental limitation: they all operate under implicit Euclidean assumptions. Message aggregation functions such as mean pooling, max pooling, or attention-weighted sums treat all neighborhoods uniformly without considering the local geometric structure of the graph.

\subsection{Geometric Deep Learning and Hyperbolic Networks}

Recent advances in geometric deep learning have explored non-Euclidean geometries for better representing hierarchical and scale-free structures. Hyperbolic neural networks map data to hyperbolic spaces (typically the PoincarÃ© ball or hyperboloid model) to leverage their natural capacity for embedding tree-like structures with low distortion.

Hyperbolic Graph Convolutional Networks (HGCNs) \cite{chami2019hyperbolic} perform message passing entirely in hyperbolic space, demonstrating improved performance on hierarchical datasets. However, they assume uniform negative curvature throughout the graph, which is unrealistic for real-world networks exhibiting mixed geometric properties. Our work addresses this limitation by computing and adapting to local curvature variations.

\subsection{Graph Curvature in Network Science}

Discrete curvature notions have a rich history in network science and computational geometry. Ollivier-Ricci curvature \cite{ollivier2009ricci} measures curvature through optimal transport distances between neighborhood distributions, while Forman-Ricci curvature provides a combinatorial alternative based on face counting. These measures have been successfully applied to community detection, robustness analysis, and anomaly detection in networks \cite{ni2015ricci}.

Despite their utility in network analysis, graph curvature measures have not been systematically integrated into deep learning architectures for representation learning. Our work bridges this gap by incorporating Ollivier-Ricci curvature directly into the message-passing framework of GNNs.

\subsection{Positioning of Our Work}

Our work differs from existing approaches in several key aspects. Unlike standard GNNs that ignore geometry, we explicitly model local curvature. Unlike hyperbolic GNNs that assume uniform curvature, we adapt to mixed-curvature structures. Unlike network science applications of curvature that use it for analysis, we integrate it into end-to-end learnable architectures. This positioning enables CANNs to combine the representational power of GNNs with the geometric awareness of curvature-based methods.

\section{Background: Understanding Graph Curvature}

Before presenting our method, we provide essential background on graph curvature and its geometric interpretation. Understanding these concepts is crucial for appreciating how CANNs leverage geometric information.

\subsection{Intuition: What is Curvature?}

In classical differential geometry, curvature quantifies how much a space deviates from being flat. Positive curvature indicates sphere-like geometry (e.g., the surface of a ball), negative curvature indicates saddle-like or hyperbolic geometry (e.g., a horse saddle), and zero curvature indicates flat Euclidean geometry (e.g., a plane).

For graphs, we seek a discrete analog of curvature that captures similar geometric properties. Intuitively, graph curvature should reflect local structural properties:

\begin{itemize}
   \item \textbf{Positive curvature} should correspond to densely connected regions where nodes share many common neighbors, resembling tightly-knit communities or clusters.
   
   \item \textbf{Negative curvature} should correspond to sparse, tree-like regions or bottlenecks where nodes have few common neighbors, resembling hierarchical structures or bridges between communities.
   
   \item \textbf{Zero curvature} should correspond to locally regular structures that are neither particularly dense nor sparse, resembling grid-like or lattice patterns.
\end{itemize}

\subsection{Ollivier-Ricci Curvature: A Practical Measure}

Among various discrete curvature notions, Ollivier-Ricci curvature provides an elegant formulation based on optimal transport theory. The key idea is to compare how far apart two nodes are relative to how far apart their neighborhoods are. If neighborhoods significantly overlap, the curvature is positive; if neighborhoods are distant, the curvature is negative.

\begin{definition}[Ollivier-Ricci Curvature]
Let $G = (V, E)$ be a graph with edge weights $w: E \to \mathbb{R}^+$. For an edge $(u,v) \in E$, define probability distributions $\mu_u$ and $\mu_v$ on the neighborhoods $\mathcal{N}(u)$ and $\mathcal{N}(v)$ respectively. The Ollivier-Ricci curvature of edge $(u,v)$ is:
\begin{equation}
   K(u,v) = 1 - \frac{W_1(\mu_u, \mu_v)}{d(u,v)}
\end{equation}
where $W_1(\mu_u, \mu_v)$ denotes the 1-Wasserstein distance (earth mover's distance) between the probability distributions, and $d(u,v)$ is the shortest path distance between $u$ and $v$.
\end{definition}

For adjacent nodes where $d(u,v) = 1$, this simplifies to:
\begin{equation}
   K(u,v) = 1 - W_1(\mu_u, \mu_v)
\end{equation}

The Wasserstein distance $W_1(\mu_u, \mu_v)$ measures the minimum cost of transforming distribution $\mu_u$ into distribution $\mu_v$, where the cost is determined by the graph distance between nodes. This can be computed by solving an optimal transport problem.

\textbf{Practical Interpretation:} Consider two adjacent nodes $u$ and $v$. If they share many common neighbors, the Wasserstein distance between their neighborhood distributions is small, yielding positive curvature $K(u,v) > 0$. Conversely, if they share few common neighbors, the Wasserstein distance is large, yielding negative curvature $K(u,v) < 0$.

\subsection{Node-Level Curvature Aggregation}

While edge curvature provides fine-grained geometric information, for computational efficiency we aggregate curvatures to obtain a scalar measure at each node:

\begin{equation}
   K(u) = \frac{1}{|\mathcal{N}(u)|} \sum_{v \in \mathcal{N}(u)} K(u,v)
\end{equation}

This averaging captures the predominant geometric character of the node's local neighborhood. Nodes in dense communities have predominantly positive edge curvatures and thus positive $K(u)$, while nodes in sparse tree-like regions have predominantly negative edge curvatures and thus negative $K(u)$.

\subsection{Geometric Interpretation in Real Networks}

To build intuition, consider concrete examples:

\textbf{Example 1 (Community Structure):} In a social network, nodes within a tightly-knit friend group share many mutual connections. Any two friends $u$ and $v$ have overlapping neighborhoods (their common friends), resulting in positive curvature $K(u,v) > 0$. The entire community exhibits positive curvature.

\textbf{Example 2 (Tree Structure):} In a citation network, a seminal paper $u$ is cited by many subsequent papers. If two of these citing papers $v$ and $w$ work on different topics, they share no common citations (except $u$), resulting in negative curvature $K(v,w) < 0$. The branching citation tree exhibits negative curvature.

\textbf{Example 3 (Bridge Nodes):} A node connecting two communities acts as a bottleneck. Its edges into one community may have positive curvature (dense connections within the community), while its edges to the other community may have negative curvature (sparse inter-community connections). Such nodes have mixed curvature.

These examples illustrate why modeling local curvature variations is essential for capturing the rich geometric structure of real-world graphs.

\section{Proposed Method: Curvature-Aware Neural Networks}

Having established the geometric foundations, we now present our Curvature-Aware Neural Network (CANN) architecture. We begin by articulating the core design principles, then provide detailed algorithmic descriptions suitable for implementation.

\subsection{Design Principles and Motivation}

The central motivation behind CANNs is to make the message-passing mechanism of GNNs \emph{geometry-aware}. Standard GNNs apply the same aggregation rule uniformly across all nodes:
\begin{equation}
   h_u^{(l+1)} = \sigma\left(W^{(l)} \cdot \text{AGG}\left(\{h_v^{(l)} : v \in \mathcal{N}(u)\}\right)\right)
\end{equation}
where $\text{AGG}$ is a permutation-invariant aggregation function (e.g., mean, sum, max) and $\sigma$ is a nonlinearity.

This uniform treatment fails to account for geometric diversity. Specifically:

\textbf{In densely connected regions} (positive curvature), information is well-distributed locally, and immediate neighbors provide redundant but reinforcing signals. Here, the aggregation should emphasize local consensus, as the dense connectivity ensures information has already propagated efficiently.

\textbf{In sparse, tree-like regions} (negative curvature), nodes serve as critical information bottlenecks connecting disparate parts of the graph. Here, the aggregation should carefully weight information from different branches, as each neighbor may provide unique, non-redundant information.

\textbf{In transition zones} (near-zero curvature), the optimal strategy lies between these extremes, requiring adaptive, learned behavior.

To implement this intuition, CANNs incorporate local curvature $K(u)$ into the aggregation mechanism, enabling the network to adapt its behavior based on local geometry. We achieve this through a curvature-modulated attention mechanism that learns to assign different importance weights to neighbors depending on both the node features and the local geometric context.

\subsection{Architecture Overview}

The CANN architecture consists of three main components operating in sequence:

\textbf{Component 1: Curvature Computation Module.} As a preprocessing step, we compute the Ollivier-Ricci curvature for all edges in the graph and aggregate them to obtain node-level curvatures $\{K(u) : u \in V\}$. This computation is performed once and cached, as curvature depends only on graph structure, not node features. For static graphs, this is a one-time cost; for dynamic graphs, incremental updates can be employed.

\textbf{Component 2: Curvature-Aware Aggregation Layers.} The core of our model consists of $L$ stacked message-passing layers. Each layer performs curvature-aware aggregation, where attention weights depend on both node features and local curvature. This allows the network to adapt its receptive field and information flow based on the underlying geometry.

\textbf{Component 3: Task-Specific Readout.} For node-level tasks (e.g., node classification), we directly use the final node embeddings. For graph-level tasks (e.g., graph classification), we apply a global pooling operation (e.g., mean or sum pooling) followed by a classification head.

In the following subsections, we provide detailed algorithmic descriptions of each component.

\subsection{Algorithm 1: Curvature Computation}

We first describe the computation of Ollivier-Ricci curvature as a preprocessing step. This algorithm computes edge curvatures based on neighborhood overlap, then aggregates them to node-level curvatures.

\begin{algorithm}[H]
\caption{Compute Graph Curvature (Preprocessing)}
\begin{algorithmic}[1]
\Require Graph $G = (V, E)$ with $n$ nodes and $m$ edges
\Ensure Node curvatures $\{K(u) : u \in V\}$
\State \textbf{Initialize:} Edge curvature dictionary $K_{edge} = \{\}$
\For{each edge $(u, v) \in E$}
   \State $\mathcal{N}(u) \leftarrow$ neighbors of $u$ (including $u$ itself)
   \State $\mathcal{N}(v) \leftarrow$ neighbors of $v$ (including $v$ itself)
   \State Define uniform distributions:
   \State \quad $\mu_u(x) = \frac{1}{|\mathcal{N}(u)|}$ for all $x \in \mathcal{N}(u)$
   \State \quad $\mu_v(x) = \frac{1}{|\mathcal{N}(v)|}$ for all $x \in \mathcal{N}(v)$
   \State $W_1 \leftarrow \text{ComputeWassersteinDistance}(\mu_u, \mu_v, G)$ \Comment{See Algorithm 2}
   \State $K_{edge}[(u, v)] \leftarrow 1 - W_1$ \Comment{For adjacent nodes, $d(u,v) = 1$}
\EndFor
\State \textbf{Initialize:} Node curvature dictionary $K_{node} = \{\}$
\For{each node $u \in V$}
   \State $K_{node}[u] \leftarrow \frac{1}{|\mathcal{N}(u)|} \sum_{v \in \mathcal{N}(u)} K_{edge}[(u, v)]$
\EndFor
\Return $K_{node}$
\end{algorithmic}
\end{algorithm}

\textbf{Remarks on Implementation:} 
\begin{itemize}
   \item We use uniform distributions over neighborhoods following standard practice in Ollivier-Ricci curvature computation. Alternative distributions (e.g., degree-weighted) can be employed if domain knowledge suggests it.
   \item The inclusion of $u$ in $\mathcal{N}(u)$ follows the lazy random walk interpretation, where a random walker has some probability of staying at the current node.
   \item For weighted graphs, edge weights can be incorporated into the distributions and distance computations.
\end{itemize}

\subsection{Algorithm 2: Wasserstein Distance Computation}

Computing the Wasserstein distance between neighborhood distributions is the computational bottleneck of curvature calculation. We provide two approaches: an exact optimal transport solver and an approximate Sinkhorn algorithm for efficiency.

\begin{algorithm}[H]
\caption{Compute Wasserstein-1 Distance Between Neighborhoods}
\begin{algorithmic}[1]
\Require Distributions $\mu_u$ on $\mathcal{N}(u)$, $\mu_v$ on $\mathcal{N}(v)$, Graph $G$
\Ensure Wasserstein distance $W_1(\mu_u, \mu_v)$
\State \textbf{Construct:} Cost matrix $C \in \mathbb{R}^{|\mathcal{N}(u)| \times |\mathcal{N}(v)|}$ where
\State \quad $C[i][j] = d_G(n_i^u, n_j^v)$ \Comment{Shortest path distance in graph $G$}
\State \textbf{Formulate:} Linear programming problem
\begin{align*}
   W_1 = \min_{\pi} \quad & \sum_{i,j} C[i][j] \cdot \pi[i][j] \\
   \text{subject to} \quad & \sum_j \pi[i][j] = \mu_u[i] \quad \forall i \\
   & \sum_i \pi[i][j] = \mu_v[j] \quad \forall j \\
   & \pi[i][j] \geq 0 \quad \forall i,j
\end{align*}
\State \textbf{Solve:} Use network simplex algorithm or Sinkhorn iteration (see Appendix)
\Return $W_1$
\end{algorithmic}
\end{algorithm}

\textbf{Computational Considerations:} The exact optimal transport problem can be solved in $O(|\mathcal{N}(u)| \cdot |\mathcal{N}(v)| \cdot \log(|\mathcal{N}(u)| + |\mathcal{N}(v)|))$ time using network simplex. For large neighborhoods, the Sinkhorn algorithm provides an $\epsilon$-approximation in $O(|\mathcal{N}(u)| \cdot |\mathcal{N}(v)| \cdot T)$ time where $T$ is the number of iterations (typically $T \approx 100$ suffices). We provide the Sinkhorn implementation in the Appendix.

\subsection{Algorithm 3: CANN Layer Forward Pass}

We now describe a single CANN layer, which constitutes the core of our architecture. This layer performs curvature-aware message passing by incorporating local geometry into the attention mechanism.

\begin{algorithm}[H]
\caption{Curvature-Aware Neural Network Layer}
\begin{algorithmic}[1]
\Require Node features $H^{(l)} = \{h_u^{(l)} \in \mathbb{R}^d : u \in V\}$
\Require Node curvatures $K = \{K(u) \in \mathbb{R} : u \in V\}$
\Require Learnable parameters: $W_k \in \mathbb{R}^{d \times 1}$, $W_q, $W_v$ \in \mathbb{R}^{d \times d}$, $a \in \mathbb{R}^{3d}$, $W^{(l)} \in \mathbb{R}^{d \times d}$, $b^{(l)} \in \mathbb{R}^d$
\Ensure Updated features $H^{(l+1)} = \{h_u^{(l+1)} \in \mathbb{R}^d : u \in V\}$
\For{each node $u \in V$}
   \State \textbf{Step 1: Embed curvature into feature space}
   \State $k_u \leftarrow W_k \cdot K(u)$ \Comment{$k_u \in \mathbb{R}^d$}
   \State
   \State \textbf{Step 2: Compute curvature-aware attention scores}
   \State Initialize attention logits: $e_{uv} \leftarrow \{\}$ for all $v \in \mathcal{N}(u)$
   \For{each neighbor $v \in \mathcal{N}(u)$}
       \State $q_u \leftarrow W_q \cdot h_u^{(l)}$ \Comment{Query from current node}
       \State $v_v \leftarrow W_v \cdot h_v^{(l)}$ \Comment{Value from neighbor}
       \State $\text{concat} \leftarrow [k_u \| q_u \| v_v]$ \Comment{Concatenate: $\in \mathbb{R}^{3d}$}
       \State $e_{uv} \leftarrow \text{LeakyReLU}(a^T \cdot \text{concat})$ \Comment{Scalar attention logit}
   \EndFor
   \State
   \State \textbf{Step 3: Normalize attention weights via softmax}
   \State $\alpha_{uv} \leftarrow \frac{\exp(e_{uv})}{\sum_{w \in \mathcal{N}(u)} \exp(e_{uw})}$ for all $v \in \mathcal{N}(u)$
   \State
   \State \textbf{Step 4: Aggregate neighbor features with learned weights}
   \State $\text{aggregated} \leftarrow \sum_{v \in \mathcal{N}(u)} \alpha_{uv} \cdot W^{(l)} \cdot h_v^{(l)}$
   \State
   \State \textbf{Step 5: Apply nonlinearity and bias}
   \State $h_u^{(l+1)} \leftarrow \text{ReLU}(\text{aggregated} + b^{(l)})$
\EndFor
\Return $H^{(l+1)}$
\end{algorithmic}
\end{algorithm}

\textbf{Design Rationale:} The key innovation in this layer is the inclusion of curvature information $k_u$ in the attention computation. By concatenating the curvature embedding with the query and value vectors, we enable the attention mechanism to learn curvature-dependent weighting strategies. For instance, in high-curvature regions, the network might learn to assign more uniform attention weights (trusting local consensus), while in low-curvature regions it might learn to be more selective (carefully choosing which neighbors to trust).

\textbf{Comparison to Standard GAT:} If we remove the curvature embedding $k_u$, this layer reduces to a standard Graph Attention Network layer. Thus, CANN can be viewed as a strict generalization of GAT that additionally conditions on geometric information.

% ...
